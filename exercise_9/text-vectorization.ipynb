{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec2cdd5",
   "metadata": {},
   "source": [
    "# テキストデータのベクトル化\n",
    "\n",
    "このノートブックの内容は斎藤（2018）『ゼロから作る Deep Learning 2』に基づいています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88193a7f",
   "metadata": {},
   "source": [
    "日本語や英語など、私たちが普段使っている言葉を自然言語（Natural Language）と言います。自然言語処理（Natural Language Processing）とは、自然言語を処理する分野です。\n",
    "\n",
    "自然言語処理の目標は、人の話す言葉をコンピュータに理解させ、私たちにとって役に立つことをコンピュータに行わせることです。\n",
    "私たちの言葉は「文字」によって表現することができます。そして、言葉の意味は「単語」（正確には形態素）によって構成されます。そのため、自然言語をコンピュータに理解させるためには、「単語の意味」を理解させることが重要です。\n",
    "\n",
    "ここでは、単語の意味を数値で表現する手法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfaab9",
   "metadata": {},
   "source": [
    "自然言語処理の研究や応用のために目的をもって収集されたテキストデータを「コーパス」と呼びます。WikipediaやGoogle Newsなどのテキストデータや、シェイクスピアや夏目漱石などの作品群もコーパスです。\n",
    "\n",
    "コーパスはテキストデータであり、そこに含まれる文章は人によって書かれたものです。これはつまり、コーパスには自然言語に対する人の知識が含まれているということです。\n",
    "文章の書き方、単語の選び方、単語の意味などがコーパスには含まれています。\n",
    "\n",
    "テキストデータのベクトル化の目的は、人の知識が詰まったコーパスから自動的に効率よく、そのエッセンスを抽出することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dd205",
   "metadata": {},
   "source": [
    "## 簡単なコーパスの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75ab17",
   "metadata": {},
   "source": [
    "テキストデータを単語に分割し、分割した単語を単語IDのリストへ変換することで、データの前処理を行いましょう。\n",
    "\n",
    "先程モジュールを使って自動で行った単語分割を手動で行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527deca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小文字に変換\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a638cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句点の前にスペースを挿入\n",
    "text = text.replace(\".\", \" .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e857600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文を単語に分割する\n",
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のIDと単語の対応表を作る\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d2a13",
   "metadata": {},
   "source": [
    "この2つの辞書を使えば、単語から単語IDの検索と、単語IDから単語の検索ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id[\"i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf38f4",
   "metadata": {},
   "source": [
    "最後に、単語のリストを単語IDのリストに変換し、NumPy配列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed646a",
   "metadata": {},
   "source": [
    "これでコーパスの前処理は終了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上の処理を関数として実装する\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    text = text.replace(\",\", \" ,\")\n",
    "    text = text.replace(\"!\", \" !\")\n",
    "    text = text.replace(\"?\", \" ?\")\n",
    "    text = text.replace(\";\", \" ;\")\n",
    "    text = text.replace(\":\", \" :\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7f53",
   "metadata": {},
   "source": [
    "## 単語の分散表現\n",
    "\n",
    "\n",
    "次に、コーパスを使って単語の意味を抽出しましょう。具体的には、単語をベクトルで表すことを目指します。これは、自然言語処理の分野では、単語の分散表現と呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41437b8d",
   "metadata": {},
   "source": [
    "単語の分散表現に関する手法は、「単語の意味は、周囲の単語によって形成される」というアイデアに基づいています。これは、分布仮説と呼ばれるものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14e5f7",
   "metadata": {},
   "source": [
    "分布仮説では、単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されると言われています。\n",
    "\n",
    "たしかに、意味的に同じ単語は、同じような文脈で多く出現します。\n",
    "\n",
    "例えば、\n",
    "\n",
    "* I drink beer.\n",
    "* We drink wine.\n",
    "\n",
    "のようにdrinkの近くには飲み物があらわれやすいでしょう。\n",
    "\n",
    "\n",
    "* I guzzle beer.\n",
    "* We guzzle wine.\n",
    "\n",
    "のような文章では、guzzleという単語がdrinkと同じような文脈で使われていることが分かります。そして、guzzleとdrinkが近い意味の単語だということが導けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e818d28",
   "metadata": {},
   "source": [
    "## 共起行列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebded83",
   "metadata": {},
   "source": [
    "分布仮説に基づいて、単語をベクトルで表す方法を考えます。\n",
    "\n",
    "素直な方法は、周囲の単語を数えることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc101a2",
   "metadata": {},
   "source": [
    "さきほど用意したコーパスに含まれるそれぞれの単語について、そのコンテキスト（目当ての単語の周囲）に含まれる単語の頻度を数えていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefe2cd",
   "metadata": {},
   "source": [
    "例えば、youという単語に着目すると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "\n",
    "このような表現になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092ce7b",
   "metadata": {},
   "source": [
    "全ての語に対してこれを数えると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "|say|1|0|1|0|1|1|0|\n",
    "|goodbye|0|1|0|1|0|0|0|\n",
    "|and|0|0|1|0|1|0|0|\n",
    "|i|0|1|0|1|0|0|0|\n",
    "|hello|0|1|0|0|0|0|1|\n",
    "|.|0|0|0|0|0|1|0|\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f10b25",
   "metadata": {},
   "source": [
    "これをNumPy配列にすることで、共起行列ができます。この共起行列を使うと、各単語の分散表現が求められます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youの分散表現\n",
    "print(id_to_word[0], C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# andの分散表現\n",
    "print(id_to_word[3], C[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上の処理を関数として実装する\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb619b8",
   "metadata": {},
   "source": [
    "単語の分散表現を使うことで、単語の類似度を計算するなど、より高度な処理ができるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d6a92",
   "metadata": {},
   "source": [
    "## ベクトル間の類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90973058",
   "metadata": {},
   "source": [
    "単語のベクトル表現の類似度の指標としては、コサイン類似度がよく使われます。コサイン類似度は、$v = (v_1, v_2, v_3, ..., v_n)$と$w = (w_1, w_2, w_3, ..., w_n)$の2つのベクトルがあるとき、次のように定義されます。\n",
    "\n",
    "$$\n",
    "cos(v, w) = \\frac{v \\cdot w}{|v||w|} = \\frac{x_{1}y_{1} + \\cdots + x_{n}y_{n}}{\\sqrt{x_1^2 + \\cdots + x_n^2}\\sqrt{y_1^2 + \\cdots + y_n^2}}\n",
    "$$\n",
    "\n",
    "コサイン類似度は、直感的には「2つのベクトルがどれだけ同じ方向を向いているか」を表します。2つのベクトルが完全に同じ方向を向いているときコサイン類似度は1になり、完全に逆向きだと-1になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3362ca6",
   "metadata": {},
   "source": [
    "コサイン類似度を実装すると、次のようになります。ただ、ゼロベクトル（ベクトルの要素がすべて0のベクトル）が引数に入ると「0除算」が発生してしまいます。\n",
    "\n",
    "この問題には、除算を行う彩に小さな値（eps）を加算することで対応します。デフォルト値として1e-8（0.00000001）を設定しましたが、これぐらい小さな値であれば、通常は浮動小数点の「丸め誤差」により他の値に吸収され、最終的な計算結果には影響を与えません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3961eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5cf8a",
   "metadata": {},
   "source": [
    "この関数を用いると、単語ベクトルの類似度を次のように求めることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed548a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82325f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = C[word_to_id['you']] # youのベクトル\n",
    "c1 = C[word_to_id['i']] # iのベクトル\n",
    "c2 = C[word_to_id['hello']] # goodbyeのベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(c0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04071a",
   "metadata": {},
   "source": [
    "上記結果から、youとiのコサイン類似度は0.70...であり、iとhelloのコサイン類似度は0.49...であることが分かりました。\n",
    "\n",
    "iはhelloよりyouとより近い語であると言えそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021f243",
   "metadata": {},
   "source": [
    "## 類似単語のランキング表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca5ca0",
   "metadata": {},
   "source": [
    "コサイン類似度を使って、便利な関数を実装しましょう。ある単語がクエリとして与えられたときに、そのクエリに対して類似した単語を上位から順に表示する関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f981c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "\n",
    "    # クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print(f\"{query} is not found\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n[query] {query}\")\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # コサイン類似度の結果から、その値を高い順に出力\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(f\"{id_to_word[i]}: {similarity[i]}\")\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2370de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893caaca",
   "metadata": {},
   "source": [
    "これを使うと、youに近い語が上から順番に出力されます。\n",
    "\n",
    "iとyouは共に人称代名詞なので類似しているのは納得できますが、helloやgoodbyeとの類似度が高いのは不思議です。実はこれはコーパスのサイズが極端に小さいことに起因します。\n",
    "\n",
    "もう少し大きいコーパスを使ってこの関数を試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421e933",
   "metadata": {},
   "source": [
    "## 少し大きいコーパスで試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3190501",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In a little district west of Washington Square the streets have run crazy and broken themselves into small strips called \"places.\" These \"places\" make strange angles and curves. One Street crosses itself a time or two. An artist once discovered a valuable possibility in this street. Suppose a collector with a bill for paints, paper and canvas should, in traversing this route, suddenly meet himself coming back, without a cent having been paid on account!\n",
    "\n",
    "So, to quaint old Greenwich Village the art people soon came prowling, hunting for north windows and eighteenth-century gables and Dutch attics and low rents. Then they imported some pewter mugs and a chafing dish or two from Sixth Avenue, and became a \"colony.\"\n",
    "\n",
    "At the top of a squatty, three-story brick Sue and Johnsy had their studio. \"Johnsy\" was familiar for Joanna. One was from Maine; the other from California. They had met at the table d'hôte of an Eighth Street \"Delmonico's,\" and found their tastes in art, chicory salad and bishop sleeves so congenial that the joint studio resulted.\n",
    "\n",
    "That was in May. In November a cold, unseen stranger, whom the doctors called Pneumonia, stalked about the colony, touching one here and there with his icy fingers. Over on the east side this ravager strode boldly, smiting his victims by scores, but his feet trod slowly through the maze of the narrow and moss-grown \"places.\"\n",
    "\n",
    "Mr. Pneumonia was not what you would call a chivalric old gentleman. A mite of a little woman with blood thinned by California zephyrs was hardly fair game for the red-fisted, short-breathed old duffer. But Johnsy he smote; and she lay, scarcely moving, on her painted iron bedstead, looking through the small Dutch window-panes at the blank side of the next brick house.\n",
    "\n",
    "One morning the busy doctor invited Sue into the hallway with a shaggy, grey eyebrow.\n",
    "\n",
    "\"She has one chance in - let us say, ten,\" he said, as he shook down the mercury in his clinical thermometer. \" And that chance is for her to want to live. This way people have of lining-u on the side of the undertaker makes the entire pharmacopoeia look silly. Your little lady has made up her mind that she's not going to get well. Has she anything on her mind?\"\n",
    "\n",
    "\"She - she wanted to paint the Bay of Naples some day.\" said Sue.\n",
    "\n",
    "\"Paint? - bosh! Has she anything on her mind worth thinking twice - a man for instance?\"\n",
    "\n",
    "\"A man?\" said Sue, with a jew's-harp twang in her voice. \"Is a man worth - but, no, doctor; there is nothing of the kind.\"\n",
    "\n",
    "\"Well, it is the weakness, then,\" said the doctor. \"I will do all that science, so far as it may filter through my efforts, can accomplish. But whenever my patient begins to count the carriages in her funeral procession I subtract 50 per cent from the curative power of medicines. If you will get her to ask one question about the new winter styles in cloak sleeves I will promise you a one-in-five chance for her, instead of one in ten.\"\n",
    "\n",
    "After the doctor had gone Sue went into the workroom and cried a Japanese napkin to a pulp. Then she swaggered into Johnsy's room with her drawing board, whistling ragtime.\n",
    "\n",
    "Johnsy lay, scarcely making a ripple under the bedclothes, with her face toward the window. Sue stopped whistling, thinking she was asleep.\n",
    "\n",
    "She arranged her board and began a pen-and-ink drawing to illustrate a magazine story. Young artists must pave their way to Art by drawing pictures for magazine stories that young authors write to pave their way to Literature.\n",
    "\n",
    "As Sue was sketching a pair of elegant horseshow riding trousers and a monocle of the figure of the hero, an Idaho cowboy, she heard a low sound, several times repeated. She went quickly to the bedside.\n",
    "\n",
    "Johnsy's eyes were open wide. She was looking out the window and counting - counting backward.\n",
    "\n",
    "\"Twelve,\" she said, and little later \"eleven\"; and then \"ten,\" and \"nine\"; and then \"eight\" and \"seven\", almost together.\n",
    "\n",
    "Sue look solicitously out of the window. What was there to count? There was only a bare, dreary yard to be seen, and the blank side of the brick house twenty feet away. An old, old ivy vine, gnarled and decayed at the roots, climbed half way up the brick wall. The cold breath of autumn had stricken its leaves from the vine until its skeleton branches clung, almost bare, to the crumbling bricks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce55777",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99406b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('say', word_to_id, id_to_word, C, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
