{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec2cdd5",
   "metadata": {},
   "source": [
    "# テキストデータの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e909",
   "metadata": {},
   "source": [
    "テキストデータを分析する際には、基本的に以下の処理が行われます。\n",
    "\n",
    "1. 形態素解析\n",
    "    * 単語分割\n",
    "    * 品詞付与\n",
    "    * 原形抽出\n",
    "2. 構文解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5c429",
   "metadata": {},
   "source": [
    "日本語を例に、`spaCy`と`GiNZA`というライブラリを使って処理の過程を見ていきましょう。\n",
    "\n",
    "`spaCy`では上の一連の処理をまとめて行ってくれます。\n",
    "\n",
    "テキストデータとして、オー・ヘンリー（結城浩訳）[『最後の一枚の葉』](https://www.hyuki.com/trans/leaf.html)の冒頭部分を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e459e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ワシントン・スクエア西にある小地区は、 道路が狂ったように入り組んでおり、 「プレース」と呼ばれる区域に小さく分かれておりました。 この「プレース」は不可思議な角度と曲線を描いており、 一、二回自分自身と交差している通りがあるほどでした。 かつて、ある画家は、この通りが貴重な可能性を持っていることを発見しました。 例えば絵や紙やキャンバスの請求書を手にした取り立て屋を考えてみてください。 取り立て屋は、この道を歩き回ったあげく、 ぐるりと元のところまで戻ってくるに違いありません。 一セントも取り立てることができずにね。\n",
    "それで、芸術家たちはまもなく、奇妙で古いグリニッチ・ヴィレッジへとやってきました。 そして、北向きの窓と十八世紀の切り妻とオランダ風の屋根裏部屋と安い賃貸料を探してうろついたのです。 やがて、彼らは しろめ製のマグやこんろ付き卓上なべを一、二個、六番街から持ち込み、 「コロニー」を形成することになりました。\n",
    "ずんぐりした三階建ての煉瓦造りの最上階では、スーとジョンジーがアトリエを持っていました。 「ジョンジー」はジョアンナの愛称です。 スーはメイン州の、ジョンジーはカリフォルニア州の出身でした。 二人は八番街の「デルモニコの店」の定食で出会い、 芸術と、チコリーのサラダと、ビショップ・スリーブの趣味がぴったりだとわかって、 共同のアトリエを持つことになったのでした。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54829e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインストール（初回のみ）\n",
    "\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import spacy\n",
    "\n",
    "# 日本語モデルのロード\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "# 解析\n",
    "doc = nlp(text)\n",
    "\n",
    "# 結果の確認\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86492713",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bd3e7",
   "metadata": {},
   "source": [
    "構文解析の結果は、次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 解析結果をpandasのDataFrameに入れる\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": token.text,\n",
    "    \"lemma_\": token.lemma_,\n",
    "    \"pos_\": token.pos_,\n",
    "    \"tag_\": token.tag_,\n",
    "    \"dep_\": token.dep_,\n",
    "    \"children\": list(token.children)\n",
    "} for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d99751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受けの図を表示する\n",
    "\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db53efd",
   "metadata": {},
   "source": [
    "前処理が完了したところで、データの確認として単語の使用頻度を数えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88206e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135cab1",
   "metadata": {},
   "source": [
    "句読点や助詞など、意味がなさそうな言葉ばかりです。\n",
    "\n",
    "より意味がある語を取り出すために、分析対象とする品詞を指定しましょう。具体的には、内容語である名詞、動詞、形容詞（、固有名詞）を指定すればよいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞の指定\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc if token.pos_ in include_pos)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897620e",
   "metadata": {},
   "source": [
    "ちょっとよくなりました。でも、「こと」「ある」「いる」などの一般的な名詞や動詞が多いように思えます。\n",
    "\n",
    "これらを不要語として指定し、除去しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b29334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"おる\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc\n",
    "                  if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84de40a",
   "metadata": {},
   "source": [
    "ずっと良くなりました。\n",
    "\n",
    "これは、語の出現頻度を要素としたテキストデータのベクトル表現で、Bag of Wordsと呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c1cd7",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "Bag of Wordsは、「多く出現する語ほど重要である」という直観的な考え方に基づいてテキストデータの特徴を表現したベクトルであり、文書分類を含む多くの自然言語処理タスクに有用です。\n",
    "\n",
    "しかし、いろいろな文書に多く出現すると予測される「わたし」などの言葉は、出現頻度のわりにさほど重要ではないと考えられます。今回の処理のようにこのような言葉を不要語として指定し、分析から除外することもできますが、この点を考慮した単語の重要度の指標として、tf-idfがあります。\n",
    "\n",
    "tf-idfは、term frequency（単語頻度）-inverse document frequency（逆文書頻度）の略です。具体的には、「ある文書内である単語がどれくらい多い頻度で出現するか」を表すterm frequencyと、「全文書内である単語を含む文書がどれくらい少ない頻度で出現するか」を表すinverse document frequencyをかけ合わせた値です。どの文書にもよく出てくる単語の重要度を下げて、あまり出てこない単語の重要度を上げるために工夫された指標です。\n",
    "\n",
    "$$\n",
    "tf-idf = tf（単語頻度） \\times \\frac{N（文書総数）}{n（単語が出現する文書数）}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "tf-idfはヒューリスティックな指標で理論的な根拠はあまりありませんが、重要語を上手く抽出でき、文書を特徴づけることができることが知られています。\n",
    "\n",
    "そのため、頻度の代わりにtf-idfを要素としてテキストデータをベクトル化することがよく行われています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
